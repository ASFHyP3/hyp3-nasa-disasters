name: Process and Transfer

on:
  schedule:
    # Run every six hours, on the hour
    - cron: '0 */6 * * *'
  workflow_dispatch:

env:
  AWS_DEFAULT_REGION: us-west-2
  AWS_ACCESS_KEY_ID: ${{ secrets.V2_AWS_ACCESS_KEY_ID }}
  AWS_SECRET_ACCESS_KEY: ${{ secrets.V2_AWS_SECRET_ACCESS_KEY }}

jobs:
  process-and-transfer:
    runs-on: ubuntu-latest

    strategy:
      fail-fast: false
      matrix:
        include:
          - environment: Hurricanes
            config_file: data_management/hurricanes.json
          - environment: Floods
            config_file: data_management/floods.json
          - environment: PDC Brazil
            config_file: data_management/pdc_brazil.json
          - environment: Alaska
            config_file: data_management/alaska.json
          - environment: Ukraine
            config_file: data_management/nas_ukraine.json
          - environment: HKHwatermaps
            config_file: data_management/hkh_watermaps.json
          - environment: Wildfires
            config_file: data_management/nm_wildfires.json

    environment:
      name: ${{ matrix.environment }}

    env:
      EDL_USERNAME: ${{ secrets.EARTHDATA_LOGIN_USER }}
      EDL_PASSWORD: ${{ secrets.EARTHDATA_LOGIN_PASSWORD }}

    steps:
      - uses: actions/checkout@v2

      - uses: conda-incubator/setup-miniconda@v2
        with:
          mamba-version: "*"
          python-version: 3.9
          activate-environment: hyp3-image-services
          environment-file: data_management/environment.yml

      - name: Process new granules
        shell: bash -l {0}
        run: |
          python data_management/process_new_granules.py -y -w ${{ matrix.config_file }}

      - name: Transfer new products
        shell: bash -l {0}
        run: |
          python data_management/hyp3_transfer_script.py -y ${{ matrix.config_file }}
